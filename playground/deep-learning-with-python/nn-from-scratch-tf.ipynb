{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        # initialize our weights\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        # make them mutable\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        # initialize our biases\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        # make them mutable\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # matmul is the dot product in tf\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index: self.index + self.batch_size]\n",
    "        labels = self.labels[self.index: self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        avg_loss = tf.reduce_mean(loss)\n",
    "    gradients = tape.gradient(avg_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return avg_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        batch_generator = BatchGenerator(images, labels, batch_size=batch_size)\n",
    "        for batch in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = training_step(model, images_batch, labels_batch)\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Batch {batch}, loss {loss:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28 * 28)\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape(test_images.shape[0], 28 * 28)\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    Dense(input_size=512, output_size=10, activation=tf.nn.softmax),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch 0, loss 0.64\n",
      "Batch 100, loss 0.66\n",
      "Batch 200, loss 0.57\n",
      "Batch 300, loss 0.63\n",
      "Batch 400, loss 0.71\n",
      "Epoch 1\n",
      "Batch 0, loss 0.61\n",
      "Batch 100, loss 0.62\n",
      "Batch 200, loss 0.54\n",
      "Batch 300, loss 0.60\n",
      "Batch 400, loss 0.68\n",
      "Epoch 2\n",
      "Batch 0, loss 0.58\n",
      "Batch 100, loss 0.59\n",
      "Batch 200, loss 0.51\n",
      "Batch 300, loss 0.57\n",
      "Batch 400, loss 0.66\n",
      "Epoch 3\n",
      "Batch 0, loss 0.56\n",
      "Batch 100, loss 0.56\n",
      "Batch 200, loss 0.48\n",
      "Batch 300, loss 0.55\n",
      "Batch 400, loss 0.64\n",
      "Epoch 4\n",
      "Batch 0, loss 0.53\n",
      "Batch 100, loss 0.53\n",
      "Batch 200, loss 0.46\n",
      "Batch 300, loss 0.53\n",
      "Batch 400, loss 0.62\n",
      "Epoch 5\n",
      "Batch 0, loss 0.52\n",
      "Batch 100, loss 0.51\n",
      "Batch 200, loss 0.45\n",
      "Batch 300, loss 0.51\n",
      "Batch 400, loss 0.61\n",
      "Epoch 6\n",
      "Batch 0, loss 0.50\n",
      "Batch 100, loss 0.49\n",
      "Batch 200, loss 0.43\n",
      "Batch 300, loss 0.50\n",
      "Batch 400, loss 0.59\n",
      "Epoch 7\n",
      "Batch 0, loss 0.49\n",
      "Batch 100, loss 0.48\n",
      "Batch 200, loss 0.42\n",
      "Batch 300, loss 0.48\n",
      "Batch 400, loss 0.58\n",
      "Epoch 8\n",
      "Batch 0, loss 0.47\n",
      "Batch 100, loss 0.46\n",
      "Batch 200, loss 0.40\n",
      "Batch 300, loss 0.47\n",
      "Batch 400, loss 0.57\n",
      "Epoch 9\n",
      "Batch 0, loss 0.46\n",
      "Batch 100, loss 0.45\n",
      "Batch 200, loss 0.39\n",
      "Batch 300, loss 0.46\n",
      "Batch 400, loss 0.56\n",
      "Epoch 10\n",
      "Batch 0, loss 0.45\n",
      "Batch 100, loss 0.44\n",
      "Batch 200, loss 0.38\n",
      "Batch 300, loss 0.45\n",
      "Batch 400, loss 0.56\n",
      "Epoch 11\n",
      "Batch 0, loss 0.44\n",
      "Batch 100, loss 0.43\n",
      "Batch 200, loss 0.37\n",
      "Batch 300, loss 0.44\n",
      "Batch 400, loss 0.55\n",
      "Epoch 12\n",
      "Batch 0, loss 0.43\n",
      "Batch 100, loss 0.42\n",
      "Batch 200, loss 0.37\n",
      "Batch 300, loss 0.43\n",
      "Batch 400, loss 0.54\n",
      "Epoch 13\n",
      "Batch 0, loss 0.42\n",
      "Batch 100, loss 0.41\n",
      "Batch 200, loss 0.36\n",
      "Batch 300, loss 0.43\n",
      "Batch 400, loss 0.54\n",
      "Epoch 14\n",
      "Batch 0, loss 0.42\n",
      "Batch 100, loss 0.40\n",
      "Batch 200, loss 0.35\n",
      "Batch 300, loss 0.42\n",
      "Batch 400, loss 0.53\n",
      "Epoch 15\n",
      "Batch 0, loss 0.41\n",
      "Batch 100, loss 0.39\n",
      "Batch 200, loss 0.35\n",
      "Batch 300, loss 0.41\n",
      "Batch 400, loss 0.52\n",
      "Epoch 16\n",
      "Batch 0, loss 0.40\n",
      "Batch 100, loss 0.38\n",
      "Batch 200, loss 0.34\n",
      "Batch 300, loss 0.41\n",
      "Batch 400, loss 0.52\n",
      "Epoch 17\n",
      "Batch 0, loss 0.40\n",
      "Batch 100, loss 0.38\n",
      "Batch 200, loss 0.33\n",
      "Batch 300, loss 0.40\n",
      "Batch 400, loss 0.51\n",
      "Epoch 18\n",
      "Batch 0, loss 0.39\n",
      "Batch 100, loss 0.37\n",
      "Batch 200, loss 0.33\n",
      "Batch 300, loss 0.40\n",
      "Batch 400, loss 0.51\n",
      "Epoch 19\n",
      "Batch 0, loss 0.39\n",
      "Batch 100, loss 0.36\n",
      "Batch 200, loss 0.32\n",
      "Batch 300, loss 0.39\n",
      "Batch 400, loss 0.51\n",
      "Epoch 20\n",
      "Batch 0, loss 0.38\n",
      "Batch 100, loss 0.36\n",
      "Batch 200, loss 0.32\n",
      "Batch 300, loss 0.39\n",
      "Batch 400, loss 0.50\n",
      "Epoch 21\n",
      "Batch 0, loss 0.38\n",
      "Batch 100, loss 0.35\n",
      "Batch 200, loss 0.32\n",
      "Batch 300, loss 0.38\n",
      "Batch 400, loss 0.50\n",
      "Epoch 22\n",
      "Batch 0, loss 0.37\n",
      "Batch 100, loss 0.35\n",
      "Batch 200, loss 0.31\n",
      "Batch 300, loss 0.38\n",
      "Batch 400, loss 0.50\n",
      "Epoch 23\n",
      "Batch 0, loss 0.37\n",
      "Batch 100, loss 0.34\n",
      "Batch 200, loss 0.31\n",
      "Batch 300, loss 0.37\n",
      "Batch 400, loss 0.49\n",
      "Epoch 24\n",
      "Batch 0, loss 0.36\n",
      "Batch 100, loss 0.34\n",
      "Batch 200, loss 0.31\n",
      "Batch 300, loss 0.37\n",
      "Batch 400, loss 0.49\n",
      "Epoch 25\n",
      "Batch 0, loss 0.36\n",
      "Batch 100, loss 0.33\n",
      "Batch 200, loss 0.30\n",
      "Batch 300, loss 0.37\n",
      "Batch 400, loss 0.49\n",
      "Epoch 26\n",
      "Batch 0, loss 0.36\n",
      "Batch 100, loss 0.33\n",
      "Batch 200, loss 0.30\n",
      "Batch 300, loss 0.37\n",
      "Batch 400, loss 0.48\n",
      "Epoch 27\n",
      "Batch 0, loss 0.35\n",
      "Batch 100, loss 0.33\n",
      "Batch 200, loss 0.30\n",
      "Batch 300, loss 0.36\n",
      "Batch 400, loss 0.48\n",
      "Epoch 28\n",
      "Batch 0, loss 0.35\n",
      "Batch 100, loss 0.32\n",
      "Batch 200, loss 0.29\n",
      "Batch 300, loss 0.36\n",
      "Batch 400, loss 0.48\n",
      "Epoch 29\n",
      "Batch 0, loss 0.35\n",
      "Batch 100, loss 0.32\n",
      "Batch 200, loss 0.29\n",
      "Batch 300, loss 0.36\n",
      "Batch 400, loss 0.48\n",
      "Epoch 30\n",
      "Batch 0, loss 0.34\n",
      "Batch 100, loss 0.32\n",
      "Batch 200, loss 0.29\n",
      "Batch 300, loss 0.35\n",
      "Batch 400, loss 0.47\n",
      "Epoch 31\n",
      "Batch 0, loss 0.34\n",
      "Batch 100, loss 0.31\n",
      "Batch 200, loss 0.29\n",
      "Batch 300, loss 0.35\n",
      "Batch 400, loss 0.47\n",
      "Epoch 32\n",
      "Batch 0, loss 0.34\n",
      "Batch 100, loss 0.31\n",
      "Batch 200, loss 0.29\n",
      "Batch 300, loss 0.35\n",
      "Batch 400, loss 0.47\n",
      "Epoch 33\n",
      "Batch 0, loss 0.33\n",
      "Batch 100, loss 0.31\n",
      "Batch 200, loss 0.28\n",
      "Batch 300, loss 0.35\n",
      "Batch 400, loss 0.47\n",
      "Epoch 34\n",
      "Batch 0, loss 0.33\n",
      "Batch 100, loss 0.30\n",
      "Batch 200, loss 0.28\n",
      "Batch 300, loss 0.35\n",
      "Batch 400, loss 0.46\n",
      "Epoch 35\n",
      "Batch 0, loss 0.33\n",
      "Batch 100, loss 0.30\n",
      "Batch 200, loss 0.28\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.46\n",
      "Epoch 36\n",
      "Batch 0, loss 0.33\n",
      "Batch 100, loss 0.30\n",
      "Batch 200, loss 0.28\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.46\n",
      "Epoch 37\n",
      "Batch 0, loss 0.32\n",
      "Batch 100, loss 0.30\n",
      "Batch 200, loss 0.28\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.46\n",
      "Epoch 38\n",
      "Batch 0, loss 0.32\n",
      "Batch 100, loss 0.29\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.46\n",
      "Epoch 39\n",
      "Batch 0, loss 0.32\n",
      "Batch 100, loss 0.29\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.45\n",
      "Epoch 40\n",
      "Batch 0, loss 0.32\n",
      "Batch 100, loss 0.29\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.34\n",
      "Batch 400, loss 0.45\n",
      "Epoch 41\n",
      "Batch 0, loss 0.32\n",
      "Batch 100, loss 0.29\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.45\n",
      "Epoch 42\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.29\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.45\n",
      "Epoch 43\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.45\n",
      "Epoch 44\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.27\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.45\n",
      "Epoch 45\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.26\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.44\n",
      "Epoch 46\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.26\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.44\n",
      "Epoch 47\n",
      "Batch 0, loss 0.31\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.26\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.44\n",
      "Epoch 48\n",
      "Batch 0, loss 0.30\n",
      "Batch 100, loss 0.28\n",
      "Batch 200, loss 0.26\n",
      "Batch 300, loss 0.33\n",
      "Batch 400, loss 0.44\n",
      "Epoch 49\n",
      "Batch 0, loss 0.30\n",
      "Batch 100, loss 0.27\n",
      "Batch 200, loss 0.26\n",
      "Batch 300, loss 0.32\n",
      "Batch 400, loss 0.44\n"
     ]
    }
   ],
   "source": [
    "fit(model, train_images, train_labels, epochs=50, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"Accuracy: {matches.mean():.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
